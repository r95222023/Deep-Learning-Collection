{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition\n",
    "\n",
    "This notebook, adapted from Deeplearning.ai's Deep Learning course, focuses on building a face recognition system. Many of the ideas presented here are from [FaceNet](https://arxiv.org/pdf/1503.03832.pdf). In the notebook, we will also encountered [DeepFace](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf).\n",
    "# Face Recognition and Verification\n",
    "\n",
    "Face recognition problems commonly fall into one of two categories:\n",
    "\n",
    "**Face Verification:** \"Is this the claimed person?\" For example, at some airports, customs can be passed by letting a system scan a passport and then verifying that the person carrying the passport is the correct individual. A mobile phone that unlocks using a face also employs face verification. This is a 1:1 matching problem.\n",
    "\n",
    "**Face Recognition:** \"Who is this person?\" For instance, the video lecture showcased a [face recognition video](https://www.youtube.com/watch?v=wr4rx0Spihs) of Baidu employees entering the office without needing to otherwise identify themselves. This is a 1:K matching problem.\n",
    "\n",
    "FaceNet learns a neural network that encodes a face image into a vector of 128 numbers. By comparing two such vectors, it is possible to determine if two pictures are of the same person.\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "* Differentiate between face recognition and face verification\n",
    "* Implement one-shot learning to solve a face recognition problem\n",
    "* Apply the triplet loss function to learn a network's parameters in the context of face recognition\n",
    "* Explain how to pose face recognition as a binary classification problem\n",
    "* Map face images into 128-dimensional encodings using a pretrained model\n",
    "* Perform face verification and face recognition with these encodings\n",
    "\n",
    "## Channels-last notation\n",
    "\n",
    "A pre-trained model representing ConvNet activations using a \"channels last\" convention will be used.\n",
    "\n",
    "In other words, a batch of images will be of shape $(m, n_H, n_W, n_C)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# uncomment the following line to install the packages.\n",
    "# !pip install numpy Pillow pandas tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Lambda, Flatten, Dense\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import PIL\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Face Verification\n",
    "\n",
    "In Face Verification, two images are given, and it must be determined if they are of the same person. The simplest way to do this is to compare the two images pixel-by-pixel. If the distance between the raw images is below a chosen threshold, it may be the same person!\n",
    "\n",
    "<img src=\"images/pixel_comparison.png\" style=\"width:380px;height:150px;\">\n",
    "<caption><center> <u> <font> <b>Figure 1</b> </u></center></caption>\n",
    "\n",
    "Of course, this algorithm performs poorly, since the pixel values change dramatically due to variations in lighting, orientation of the person's face, minor changes in head position, and so on.\n",
    "\n",
    "Rather than using the raw image, an encoding, $f(\\text{img})$, can be learned.\n",
    "\n",
    "By using an encoding for each image, an element-wise comparison produces a more accurate judgment as to whether two pictures are of the same person.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Face Images into a 128-Dimensional Vector\n",
    "\n",
    "### Using a ConvNet to Compute Encodings\n",
    "\n",
    "The FaceNet model requires a lot of data and significant time to train. Following common practice in applied deep learning, pre-trained weights will be loaded. The network architecture follows the Inception model from [Szegedy *et al*](https://arxiv.org/abs/1409.4842). The pre-trained model can be downloaded [here](https://www.kaggle.com/datasets/rmamun/kerasfaceneth5). An Inception network implementation is provided in the file `inception_blocks_v2.py` for a closer look at its implementation.\n",
    "\n",
    "*Hot tip:* Go to \"File->Open...\" at the top of this notebook to open the file directory containing the `.py` file.\n",
    "\n",
    "Key points to note:\n",
    "\n",
    "- This network uses 160x160 dimensional RGB images as its input. Specifically, a face image (or batch of $m$ face images) is a tensor of shape $(m, n_H, n_W, n_C) = (m, 160, 160, 3)$.\n",
    "- The input images are originally of shape 96x96, thus scaling to 160x160 is required. This scaling is performed in the `img_to_encoding()` function.\n",
    "- The output is a matrix of shape $(m, 128)$ that encodes each input face image into a 128-dimensional vector.\n",
    "\n",
    "Run the cell below to create the model for face images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "# put keras-facenet-h5 in the same directory as this file\n",
    "# load json and create model\n",
    "json_file = open('keras-facenet-h5/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights('keras-facenet-h5/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now summarize the input and output shapes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'input_1:0' shape=(None, 160, 160, 3) dtype=float32>]\n",
      "[<tf.Tensor 'Bottleneck_BatchNorm/batchnorm/add_1:0' shape=(None, 128) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print(model.inputs)\n",
    "print(model.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a 128-neuron fully connected layer as its last layer, the model ensures that the output is an encoding vector of size 128. The encodings are then used to compare two face images as follows:\n",
    "\n",
    "<img src=\"images/distance_kiank.png\" style=\"width:680px;height:250px;\">\n",
    "<caption><center> <u> <font> <b>Figure 2:</b> </u> <font><br>By computing the distance between two encodings and thresholding, it can be determined if the two pictures represent the same person.</center></caption>\n",
    "\n",
    "An encoding is considered effective if:\n",
    "\n",
    "- The encodings of two images of the same person are quite similar.\n",
    "- The encodings of two images of different persons are very different.\n",
    "\n",
    "The triplet loss function formalizes this concept, aiming to \"push\" the encodings of two images of the same person (Anchor and Positive) closer together, while \"pulling\" the encodings of two images of different persons (Anchor and Negative) further apart.\n",
    "\n",
    "<img src=\"images/triplet_comparison.png\" style=\"width:280px;height:150px;\"><br>\n",
    "<caption><center> <u> <font> <b>Figure 3: </b> </u> <font><br>In the next section, the pictures from left to right are referred to as Anchor (A), Positive (P), and Negative (N).</center></caption>\n",
    "\n",
    "\n",
    "By using a 128-neuron fully connected layer as its last layer, the model ensures that the output is an encoding vector of size 128. We then use the encodings to compare two face images as follows:\n",
    "\n",
    "<img src=\"images/distance_kiank.png\" style=\"width:680px;height:250px;\">\n",
    "<caption><center> <u> <font> <b>Figure 2:</b> <br> </u> <font>By computing the distance between two encodings and thresholding, we can determine if the two pictures represent the same person</center></caption>\n",
    "\n",
    "So, an encoding is a good one if:\n",
    "\n",
    "- The encodings of two images of the same person are quite similar to each other.\n",
    "- The encodings of two images of different persons are very different.\n",
    "\n",
    "The triplet loss function formalizes this, and tries to \"push\" the encodings of two images of the same person (Anchor and Positive) closer together, while \"pulling\" the encodings of two images of different persons (Anchor, Negative) further apart.\n",
    "    \n",
    "<img src=\"images/triplet_comparison.png\" style=\"width:280px;height:150px;\"><br>\n",
    "<caption><center> <u> <font> <b>Figure 3: </b> <br> </u> <font> In the next section,  we'll call the pictures from left to right: Anchor (A), Positive (P), Negative (N)</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Triplet Loss\n",
    "\n",
    "**Important Note**: Since a pretrained model is being used, implementing the triplet loss function is not necessary. However, the triplet loss is the main ingredient of the face recognition algorithm, and understanding how to use it is essential for training a custom FaceNet model or addressing other types of image similarity problems. Therefore, the triplet loss function will be implemented below for educational purposes.\n",
    "\n",
    "For an image $ x $, its encoding is denoted as $ f(x) $, where $ f $ is the function computed by the neural network.\n",
    "\n",
    "<img src=\"images/f_x.png\" style=\"width:380px;height:150px;\">\n",
    "\n",
    "Training utilizes triplets of images $(A, P, N)$:\n",
    "\n",
    "- $ A $ is an \"Anchor\" image—a picture of a person.\n",
    "- $ P $ is a \"Positive\" image—a picture of the same person as the Anchor image.\n",
    "- $ N $ is a \"Negative\" image—a picture of a different person than the Anchor image.\n",
    "\n",
    "These triplets are selected from the training dataset. $ (A^{(i)}, P^{(i)}, N^{(i)}) $ denotes the $ i $-th training example.\n",
    "\n",
    "The goal is to ensure that an image $ A^{(i)} $ of an individual is closer to the Positive $ P^{(i)} $ than to the Negative image $ N^{(i)} $ by at least a margin $ \\alpha $:\n",
    "\n",
    "$$\n",
    "\\| f(A^{(i)}) - f(P^{(i)}) \\|_{2}^{2} + \\alpha < \\| f(A^{(i)}) - f(N^{(i)}) \\|_{2}^{2}\n",
    "$$\n",
    "\n",
    "The objective is to minimize the following \"triplet cost\":\n",
    "\n",
    "$$\\mathcal{J} = \\sum^{m}_{i=1} \\left[ \\underbrace{\\| f(A^{(i)}) - f(P^{(i)}) \\|_2^2}_\\text{(1)} - \\underbrace{\\| f(A^{(i)}) - f(N^{(i)}) \\|_2^2}_\\text{(2)} + \\alpha \\right]_+ \\tag{3}$$\n",
    "\n",
    "Here, the notation \"$[z]_+$\" denotes $ \\max(z, 0) $.\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "- The term (1) is the squared distance between the anchor \"A\" and the positive \"P\" for a given triplet; this should be small.\n",
    "- The term (2) is the squared distance between the anchor \"A\" and the negative \"N\" for a given triplet; this should be relatively large. The minus sign preceding it indicates that minimizing the negative term is equivalent to maximizing that term.\n",
    "- $ \\alpha $ is called the margin, a hyperparameter manually selected. In this case, $ \\alpha = 0.2 $ will be used.\n",
    "\n",
    "Most implementations also rescale the encoding vectors to have an L2 norm equal to one (i.e., $ \\| f(\\text{img}) \\|_2 = 1 $); this rescaling is not required.\n",
    "\n",
    "### triplet_loss\n",
    "\n",
    "The triplet loss is defined by formula (3). These are the 4 steps:\n",
    "\n",
    "1. Compute the distance between the encodings of \"anchor\" and \"positive\": $ \\| f(A^{(i)}) - f(P^{(i)}) \\|_2^2 $.\n",
    "2. Compute the distance between the encodings of \"anchor\" and \"negative\": $ \\| f(A^{(i)}) - f(N^{(i)}) \\|_2^2 $.\n",
    "3. Compute the formula per training example: $ \\| f(A^{(i)}) - f(P^{(i)}) \\|_2^2 - \\| f(A^{(i)}) - f(N^{(i)}) \\|_2^2 + \\alpha $.\n",
    "4. Compute the full formula by taking the max with zero and summing over the training examples:\n",
    "\n",
    "$$\\mathcal{J} = \\sum^{m}_{i=1} \\left[ \\| f(A^{(i)}) - f(P^{(i)}) \\|_2^2 - \\| f(A^{(i)}) - f(N^{(i)}) \\|_2^2 + \\alpha \\right]_+ \\tag{3}$$\n",
    "\n",
    "*Key Points*:\n",
    "\n",
    "- Useful functions: `tf.reduce_sum()`, `tf.square()`, `tf.subtract()`, `tf.add()`, `tf.maximum()`.\n",
    "\n",
    "- For steps 1 and 2, sum over the entries of $ \\| f(A^{(i)}) - f(P^{(i)}) \\|_2^2 $ and $ \\| f(A^{(i)}) - f(N^{(i)}) \\|_2^2 $.\n",
    "\n",
    "- For step 4, sum over the training examples.\n",
    "\n",
    "- Recall that the square of the L2 norm is the sum of the squared differences: $ ||x - y||_2^2 = \\sum_{i=1}^{N}(x_i - y_i)^2 $.\n",
    "\n",
    "- Note that the anchor, positive, and negative encodings are of shape $(m, 128)$, where $ m $ is the number of training examples and 128 is the number of elements used to encode a single example.\n",
    "\n",
    "- For steps 1 and 2, maintain the number of $ m $ training examples and sum along the 128 values of each encoding. `tf.reduce_sum` has an axis parameter to choose along which axis the sums are applied.\n",
    "\n",
    "- One way to choose the last axis in a tensor is to use negative indexing (axis=-1).\n",
    "\n",
    "- In step 4, when summing over training examples, the result will be a single scalar value.\n",
    "\n",
    "- For `tf.reduce_sum` to sum across all axes, keep the default value axis=None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f05732f7068382cb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "\n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),axis=-1)\n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis=-1)\n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.maximum(tf.add(tf.subtract(pos_dist,neg_dist),alpha),0)\n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(basic_loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-440ff81e6bcda96a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = tf.Tensor(527.2598, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNIT TEST\n",
    "tf.random.set_seed(1)\n",
    "y_true = (None, None, None) # It is not used\n",
    "y_pred = (tf.keras.backend.random_normal([3, 128], mean=6, stddev=0.1, seed = 1),\n",
    "          tf.keras.backend.random_normal([3, 128], mean=1, stddev=1, seed = 1),\n",
    "          tf.keras.backend.random_normal([3, 128], mean=3, stddev=4, seed = 1))\n",
    "loss = triplet_loss(y_true, y_pred)\n",
    "\n",
    "assert type(loss) == tf.python.framework.ops.EagerTensor, \"Use tensorflow functions\"\n",
    "print(\"loss = \" + str(loss))\n",
    "\n",
    "y_pred_perfect = ([[1., 1.]], [[1., 1.]], [[1., 1.,]])\n",
    "loss = triplet_loss(y_true, y_pred_perfect, 5)\n",
    "assert loss == 5, \"Wrong value. Did you add the alpha to basic_loss?\"\n",
    "y_pred_perfect = ([[1., 1.]],[[1., 1.]], [[0., 0.,]])\n",
    "loss = triplet_loss(y_true, y_pred_perfect, 3)\n",
    "assert loss == 1., \"Wrong value. Check that pos_dist = 0 and neg_dist = 2 in this example\"\n",
    "y_pred_perfect = ([[1., 1.]],[[0., 0.]], [[1., 1.,]])\n",
    "loss = triplet_loss(y_true, y_pred_perfect, 0)\n",
    "assert loss == 2., \"Wrong value. Check that pos_dist = 2 and neg_dist = 0 in this example\"\n",
    "y_pred_perfect = ([[0., 0.]],[[0., 0.]], [[0., 0.,]])\n",
    "loss = triplet_loss(y_true, y_pred_perfect, -2)\n",
    "assert loss == 0, \"Wrong value. Are you taking the maximum between basic_loss and 0?\"\n",
    "y_pred_perfect = ([[1., 0.], [1., 0.]],[[1., 0.], [1., 0.]], [[0., 1.], [0., 1.]])\n",
    "loss = triplet_loss(y_true, y_pred_perfect, 3)\n",
    "assert loss == 2., \"Wrong value. Are you applying tf.reduce_sum to get the loss?\"\n",
    "y_pred_perfect = ([[1., 1.], [2., 0.]], [[0., 3.], [1., 1.]], [[1., 0.], [0., 1.,]])\n",
    "loss = triplet_loss(y_true, y_pred_perfect, 1)\n",
    "if (loss == 4.):\n",
    "    raise Exception('Perhaps you are not using axis=-1 in reduce_sum?')\n",
    "assert loss == 5, \"Wrong value. Check your implementation\"\n",
    "\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>loss:</b>\n",
    "        </td>\n",
    "        <td>\n",
    "           tf.Tensor(527.2598, shape=(), dtype=float32)\n",
    "        </td>\n",
    "    </tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pre-trained Model\n",
    "\n",
    "FaceNet is trained by minimizing the triplet loss. But since training requires a lot of data and a lot of computation, we will load a previously trained model in the following cell; which might take a couple of minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-953bcab8e9bbba10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "FRmodel = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of distances between the encodings between three individuals:\n",
    "\n",
    "<img src=\"images/distance_matrix.png\" style=\"width:380px;height:200px;\"><br>\n",
    "<caption><center> <u> <font> <b>Figure 4:</b></u> <br>  <font> Example of distance outputs between three individuals' encodings</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Model\n",
    "\n",
    "A system is being developed for an office building where the building manager aims to implement facial recognition for employees to enter the premises.\n",
    "\n",
    "The goal is to create a face verification system that grants access to a list of authorized individuals. Upon arrival, each person swipes an identification card at the entrance. The face recognition system then verifies their identity.\n",
    "\n",
    "### Face Verification\n",
    "\n",
    "A database will be constructed containing one encoding vector for each person authorized to enter the office. To generate the encoding, the function `img_to_encoding(image_path, model)` is used, which runs the model's forward propagation on the specified image.\n",
    "\n",
    "Execute the following code to build the database (represented as a Python dictionary). This database maps each person's name to a 128-dimensional encoding of their face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.set_image_data_format('channels_last')\n",
    "def img_to_encoding(image_path, model):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(160, 160))\n",
    "    img = np.around(np.array(img) / 255.0, decimals=12)\n",
    "    x_train = np.expand_dims(img, axis=0)\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    return embedding / np.linalg.norm(embedding, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"danielle\"] = img_to_encoding(\"images/danielle.png\", FRmodel)\n",
    "database[\"younes\"] = img_to_encoding(\"images/younes.jpg\", FRmodel)\n",
    "database[\"tian\"] = img_to_encoding(\"images/tian.jpg\", FRmodel)\n",
    "database[\"andrew\"] = img_to_encoding(\"images/andrew.jpg\", FRmodel)\n",
    "database[\"kian\"] = img_to_encoding(\"images/kian.jpg\", FRmodel)\n",
    "database[\"dan\"] = img_to_encoding(\"images/dan.jpg\", FRmodel)\n",
    "database[\"sebastiano\"] = img_to_encoding(\"images/sebastiano.jpg\", FRmodel)\n",
    "database[\"bertrand\"] = img_to_encoding(\"images/bertrand.jpg\", FRmodel)\n",
    "database[\"kevin\"] = img_to_encoding(\"images/kevin.jpg\", FRmodel)\n",
    "database[\"felix\"] = img_to_encoding(\"images/felix.jpg\", FRmodel)\n",
    "database[\"benoit\"] = img_to_encoding(\"images/benoit.jpg\", FRmodel)\n",
    "database[\"arnaud\"] = img_to_encoding(\"images/arnaud.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "danielle = tf.keras.preprocessing.image.load_img(\"images/danielle.png\", target_size=(160, 160))\n",
    "kian = tf.keras.preprocessing.image.load_img(\"images/kian.jpg\", target_size=(160, 160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 160, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(np.array(kian) / 255.0, decimals=12).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 160, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(np.array(danielle) / 255.0, decimals=12).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify\n",
    "\n",
    "The `verify()` function checks if the front-door camera picture (`image_path`) matches the identity of the person named \"identity\". It follows these steps to accomplish this:\n",
    "\n",
    "- Compute the encoding of the image from `image_path`.\n",
    "- Compute the distance between this encoding and the encoding of the identity image stored in the database.\n",
    "- Open the door if the distance is less than 0.7; otherwise, do not open it.\n",
    "\n",
    "As discussed, use the L2 distance `np.linalg.norm` for this comparison.\n",
    "\n",
    "**Note**: Compare the L2 distance, not the square of the L2 distance, to the threshold of 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba2f317e79e15a2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def verify(image_path, identity, database, model):\n",
    "    \"\"\"\n",
    "    Function that verifies if the person on the \"image_path\" image is \"identity\".\n",
    "    \n",
    "    Arguments:\n",
    "        image_path -- path to an image\n",
    "        identity -- string, name of the person you'd like to verify the identity. Has to be an employee who works in the office.\n",
    "        database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
    "        model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "        dist -- distance between the image_path and the image of \"identity\" in the database.\n",
    "        door_open -- True, if the door should open. False otherwise.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above.\n",
    "    encoding = img_to_encoding(image_path,model)\n",
    "    # Step 2: Compute distance with identity's image\n",
    "    dist = np.linalg.norm(encoding - database[identity])\n",
    "    # Step 3: Open the door if dist < 0.7, else don't open\n",
    "    if dist < 0.7:\n",
    "        print(\"It's \" + str(identity) + \", welcome in!\")\n",
    "        door_open = True\n",
    "    else:\n",
    "        print(\"It's not \" + str(identity) + \", please go away\")\n",
    "        door_open = False     \n",
    "    return dist, door_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the `verify` function on some pictures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-014d077254ad7d52",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "distance, door_open_flag = verify(\"images/camera_0.jpg\", \"younes\", database, FRmodel)\n",
    "assert np.isclose(distance, 0.5992949), \"Distance not as expected\"\n",
    "assert isinstance(door_open_flag, bool), \"Door open flag should be a boolean\"\n",
    "print(\"(\", distance, \",\", door_open_flag, \")\")\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>It's younes, welcome in!</b>\n",
    "        </td>\n",
    "        <td>\n",
    "           (0.5992949, True)\n",
    "        </td>\n",
    "    </tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's not kian, please go away\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0259346, False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_2.jpg\", \"kian\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>It's not kian, please go away</b>\n",
    "        </td>\n",
    "        <td>\n",
    "           (1.0259346, False)\n",
    "        </td>\n",
    "    </tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Recognition\n",
    "\n",
    "The current face verification system is functioning, but an issue arose when Kian had his ID card stolen. The next day, he was unable to access the building without his card.\n",
    "\n",
    "To address this problem, the system will be upgraded to face recognition. This change eliminates the need for ID cards, allowing authorized individuals to simply walk up to the building and have the door unlock automatically.\n",
    "\n",
    "The new face recognition system will identify if an image belongs to one of the authorized persons and determine their identity. Unlike the previous verification system, the input will not include a person's name.\n",
    "\n",
    "### who_is_it\n",
    "\n",
    "The `who_is_it()` function has the following steps:\n",
    "\n",
    "- Compute the encoding for the image provided by `image_path`.\n",
    "- Identify the encoding in the database that has the smallest distance to the target encoding.\n",
    "- Initialize the `min_dist` variable to a sufficiently large number (e.g., 100) to track the closest encoding.\n",
    "- Iterate over the database dictionary's names and encodings using `for (name, db_enc) in database.items()`.\n",
    "- Calculate the L2 distance between the target encoding and each encoding from the database. If this distance is smaller than `min_dist`, update `min_dist` and set `identity` to the current `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a04ff2b5fd1186f8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def who_is_it(image_path, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the office by finding who is the person on the image_path image.\n",
    "    \n",
    "    Arguments:\n",
    "        image_path -- path to an image\n",
    "        database -- database containing image encodings along with the name of the person on the image\n",
    "        model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "        min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "        identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "\n",
    "    ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. \n",
    "    encoding = img_to_encoding(image_path,model)\n",
    "    \n",
    "    ## Step 2: Find the closest encoding ##\n",
    "    \n",
    "    # Initialize \"min_dist\" to a large value, say 100\n",
    "    min_dist = 100\n",
    "    \n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_enc) in database.items():\n",
    "        \n",
    "        # Compute L2 distance between the target \"encoding\" and the current db_enc from the database. \n",
    "        dist = np.linalg.norm(encoding - db_enc)\n",
    "\n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. \n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "    \n",
    "    if min_dist > 0.7:\n",
    "        print(\"Not in the database.\")\n",
    "    else:\n",
    "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
    "        \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Younes is at the front door and the camera takes a picture of him (\"images/camera_0.jpg\"). Let's see if your `who_it_is()` algorithm identifies Younes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9c88c8ab87677503",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's younes, the distance is 0.5992949\n",
      "it's younes, the distance is 0.5992949\n",
      "it's younes, the distance is 0.0\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNIT TEST\n",
    "# Test 1 with Younes pictures \n",
    "who_is_it(\"images/camera_0.jpg\", database, FRmodel)\n",
    "\n",
    "# Test 2 with Younes pictures \n",
    "test1 = who_is_it(\"images/camera_0.jpg\", database, FRmodel)\n",
    "assert np.isclose(test1[0], 0.5992946)\n",
    "assert test1[1] == 'younes'\n",
    "\n",
    "# Test 3 with Younes pictures \n",
    "test2 = who_is_it(\"images/younes.jpg\", database, FRmodel)\n",
    "assert np.isclose(test2[0], 0.0)\n",
    "assert test2[1] == 'younes'\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>it's younes, the distance is</b> 0.5992949<br>\n",
    "            <b>it's younes, the distance is</b> 0.5992949<br>\n",
    "            <b>it's younes, the distance is</b> 0.0<br>\n",
    "        </td>\n",
    "    </tr>\n",
    "    </table>\n",
    "\n",
    " Change \"camera_0.jpg\" (picture of Younes) to \"camera_1.jpg\" (picture of Bertrand) and see the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap: \n",
    "\n",
    "- Posed face recognition as a binary classification problem\n",
    "- Implemented one-shot learning for a face recognition problem\n",
    "- Applied the triplet loss function to learn a network's parameters in the context of face recognition\n",
    "- Mapped face images into 128-dimensional encodings using a pretrained model\n",
    "- Performed face verification and face recognition with these encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enhancing the Model**:\n",
    "\n",
    "Several strategies can be employed to further enhance the performance of the algorithm:\n",
    "\n",
    "- **Expand the Database**: Include additional images of each person, taken under various lighting conditions and on different days. Comparing new images against a larger set of photos for each individual can significantly improve accuracy.\n",
    "\n",
    "- **Crop the Images**: Focus the images on the face itself, minimizing the \"border\" region around the face. This preprocessing step reduces irrelevant pixels and enhances the algorithm's robustness by concentrating on the key features of the face.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "\n",
    "2. Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf)\n",
    "\n",
    "3. This implementation also took a lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet\n",
    "\n",
    "4. Further inspiration was found here: https://machinelearningmastery.com/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier/\n",
    "\n",
    "5. And here: https://github.com/nyoki-mtl/keras-facenet/blob/master/notebook/tf_to_keras.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
