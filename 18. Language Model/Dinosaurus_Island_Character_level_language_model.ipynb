{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qo4gVJ_D01SH"
   },
   "source": [
    "# Character level language model - Dinosaurus Island\n",
    "\n",
    "This notebook, adapted from Deeplearning.ai's Deep Learning course, \n",
    "builds a character-level language model to give names to dinosaurs.\n",
    "\n",
    "<!-- <table>\n",
    "<td>\n",
    "<img src=\"images/dino.jpg\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "</table> -->\n",
    "\n",
    "A list of all the dinosaur names are compiled into this [dataset](dinos.txt). To create new dinosaur names, the algorithm will learn the different name patterns, and randomly generate new names.\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* Store text data for processing using an RNN \n",
    "* Build a character-level text generation model using an RNN\n",
    "* Sample novel sequences in an RNN\n",
    "* Explain the vanishing/exploding gradient problem in RNNs\n",
    "* Apply gradient clipping as a solution for exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# uncomment the following line to install the packages.\n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0Nj4psY01SJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Functions such as `rnn_forward` and `rnn_backward` are provided in `rnn_utils`\n",
    "from utils import *\n",
    "import random\n",
    "import pprint\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_elAxqq01SN"
   },
   "source": [
    "### Dataset and Preprocessing\n",
    "\n",
    "Run the following cell to read the dataset of dinosaur names, create a list of unique characters (such as a-z), and compute the dataset and vocabulary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qB2XWVg_01SO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfmL668r01SQ"
   },
   "source": [
    "\n",
    "* The characters are a-z (26 characters) plus the \"\\n\" (or newline character).\n",
    "* The newline character \"\\n\" plays a role similar to the `<EOS>` (or \"End of sentence\") token discussed in lecture.  \n",
    "    - Here, \"\\n\" indicates the end of the dinosaur name rather than the end of a sentence. \n",
    "* `char_to_ix`: In the cell below, we'll create a Python dictionary (i.e., a hash table) to map each character to an index from 0-26.\n",
    "* `ix_to_char`: Then, we'll create a second Python dictionary that maps each index back to the corresponding character. \n",
    "    -  This will clarify which index in the probability distribution output of the softmax layer corresponds to each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bh3QcYpr01SQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2YltsxeZ01SU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   0: '\\n',\n",
      "    1: 'a',\n",
      "    2: 'b',\n",
      "    3: 'c',\n",
      "    4: 'd',\n",
      "    5: 'e',\n",
      "    6: 'f',\n",
      "    7: 'g',\n",
      "    8: 'h',\n",
      "    9: 'i',\n",
      "    10: 'j',\n",
      "    11: 'k',\n",
      "    12: 'l',\n",
      "    13: 'm',\n",
      "    14: 'n',\n",
      "    15: 'o',\n",
      "    16: 'p',\n",
      "    17: 'q',\n",
      "    18: 'r',\n",
      "    19: 's',\n",
      "    20: 't',\n",
      "    21: 'u',\n",
      "    22: 'v',\n",
      "    23: 'w',\n",
      "    24: 'x',\n",
      "    25: 'y',\n",
      "    26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuDQfApB01SW"
   },
   "source": [
    "### Overview of the Model\n",
    "\n",
    "The model has the following structure:\n",
    "\n",
    "- **Initialize Parameters**: Set up initial values for model parameters.\n",
    "- **Run the Optimization Loop**:\n",
    "    - **Forward Propagation**: Compute the loss function.\n",
    "    - **Backward Propagation**: Calculate the gradients with respect to the loss function.\n",
    "    - **Gradient Clipping**: Prevent exploding gradients by clipping them.\n",
    "    - **Parameter Update**: Use the gradients to update parameters via the gradient descent rule.\n",
    "- **Return Learned Parameters**: Output the optimized parameters.\n",
    "\n",
    "<img src=\"images/rnn.png\" style=\"width:450;height:300px;\">\n",
    "<caption><center><font><b>Figure 1</b>: Recurrent Neural Network, similar to the model built in the previous notebook \"Building a Recurrent Neural Network - Step by Step.\"</center></caption>\n",
    "\n",
    "- At each time step, the RNN predicts the next character based on previous characters.\n",
    "- $\\mathbf{X} = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ represents a sequence of characters from the training set.\n",
    "- $\\mathbf{Y} = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$ is the same sequence, but shifted one character forward.\n",
    "- At each time step $t$, $y^{\\langle t \\rangle}$ equals $x^{\\langle t+1 \\rangle}$. Therefore, the prediction at time $t$ corresponds to the input at time $t + 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNFjtuyq01SW"
   },
   "source": [
    "## Building Blocks of the Model\n",
    "\n",
    "In this part, we will build two important blocks of the overall model:\n",
    "\n",
    "1. Gradient clipping: to avoid exploding gradients\n",
    "2. Sampling: a technique used to generate characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tuVLtQaM01SX"
   },
   "source": [
    "### Clipping the Gradients in the Optimization Loop\n",
    "\n",
    "This section focuses on implementing the `clip` function, which will be utilized within the optimization loop.\n",
    "\n",
    "#### Exploding Gradients\n",
    "\n",
    "- **Exploding Gradients**: When gradients become excessively large, they are referred to as \"exploding gradients.\" This issue complicates the training process because large updates can cause the model parameters to \"overshoot\" optimal values during backpropagation.\n",
    "\n",
    "Typically, the overall loop structure includes:\n",
    "- Forward pass\n",
    "- Cost computation\n",
    "- Backward pass\n",
    "- Parameter update\n",
    "\n",
    "Before updating the parameters, gradient clipping is performed to ensure that gradients do not become excessively large.\n",
    "\n",
    "#### Gradient Clipping\n",
    "\n",
    "In the following exercise, the `clip` function will be implemented to handle gradients in a dictionary, returning a clipped version if necessary.\n",
    "\n",
    "- **Gradient Clipping Methods**: Various techniques can be used to clip gradients. Here, a simple element-wise clipping approach is applied.\n",
    "- **Element-Wise Clipping**: Each element of the gradient vector is constrained within a range [-N, N]. \n",
    "    - For example, with $ N = 10 $:\n",
    "        - The clipping range is [-10, 10].\n",
    "        - Any gradient component exceeding 10 is set to 10.\n",
    "        - Any gradient component below -10 is set to -10.\n",
    "        - Gradient components between -10 and 10 retain their original values.\n",
    "\n",
    "<img src=\"images/clip.png\" style=\"width:400;height:150px;\">\n",
    "<caption><center><b>Figure 2</b>: Visualization of gradient descent with and without gradient clipping, demonstrating how gradient clipping addresses \"exploding gradient\" issues.</center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clip\n",
    "\n",
    "The `clip` function returns the clipped gradients from the dictionary `gradients`.\n",
    "\n",
    "- **Function Purpose**: The function takes a maximum threshold and applies clipping to the gradients in the dictionary.\n",
    "- **Usage of `numpy.clip`**: For more details, refer to [numpy.clip](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.clip.html).\n",
    "    - Utilize the `out` parameter to perform in-place updates.\n",
    "    - If the `out` argument is not used, the clipped gradients are computed but not applied to the gradient variables (`dWax`, `dWaa`, `dWya`, `db`, `dby`).\n",
    "\n",
    "The `clip` function ensures that the gradients do not exceed the specified threshold, maintaining stability during the optimization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yYvYeI501SX"
   },
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    gradients = copy.deepcopy(gradients)\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    # Clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby].\n",
    "    for gradient in gradients:\n",
    "        np.clip(gradients[gradient], -maxValue, maxValue, out = gradients[gradient])\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NAdY1Eon01Sa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradients for mValue=10\n",
      "gradients[\"dWaa\"][1][2] = 10.0\n",
      "gradients[\"dWax\"][3][1] = -10.0\n",
      "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
      "gradients[\"db\"][4] = [10.]\n",
      "gradients[\"dby\"][1] = [8.45833407]\n",
      "\u001b[92mAll tests passed!\u001b[0m\n",
      "\n",
      "Gradients for mValue=5\n",
      "gradients[\"dWaa\"][1][2] = 5.0\n",
      "gradients[\"dWax\"][3][1] = -5.0\n",
      "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
      "gradients[\"db\"][4] = [5.]\n",
      "gradients[\"dby\"][1] = [5.]\n",
      "\u001b[92mAll tests passed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Test with a max value of 10\n",
    "def clip_test(target, mValue):\n",
    "    print(f\"\\nGradients for mValue={mValue}\")\n",
    "    np.random.seed(3)\n",
    "    dWax = np.random.randn(5, 3) * 10\n",
    "    dWaa = np.random.randn(5, 5) * 10\n",
    "    dWya = np.random.randn(2, 5) * 10\n",
    "    db = np.random.randn(5, 1) * 10\n",
    "    dby = np.random.randn(2, 1) * 10\n",
    "    gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "\n",
    "    gradients2 = target(gradients, mValue)\n",
    "    print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients2[\"dWaa\"][1][2])\n",
    "    print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients2[\"dWax\"][3][1])\n",
    "    print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients2[\"dWya\"][1][2])\n",
    "    print(\"gradients[\\\"db\\\"][4] =\", gradients2[\"db\"][4])\n",
    "    print(\"gradients[\\\"dby\\\"][1] =\", gradients2[\"dby\"][1])\n",
    "    \n",
    "    for grad in gradients2.keys():\n",
    "        valuei = gradients[grad]\n",
    "        valuef = gradients2[grad]\n",
    "        mink = np.min(valuef)\n",
    "        maxk = np.max(valuef)\n",
    "        assert mink >= -abs(mValue), f\"Problem with {grad}. Set a_min to -mValue in the np.clip call\"\n",
    "        assert maxk <= abs(mValue), f\"Problem with {grad}.Set a_max to mValue in the np.clip call\"\n",
    "        index_not_clipped = np.logical_and(valuei <= mValue, valuei >= -mValue)\n",
    "        assert np.all(valuei[index_not_clipped] == valuef[index_not_clipped]), f\" Problem with {grad}. Some values that should not have changed, changed during the clipping process.\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\\x1b[0m\")\n",
    "    \n",
    "clip_test(clip, 10)\n",
    "clip_test(clip, 5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6O4BLjJf01Sd"
   },
   "source": [
    "**Expected values**\n",
    "```\n",
    "Gradients for mValue=10\n",
    "gradients[\"dWaa\"][1][2] = 10.0\n",
    "gradients[\"dWax\"][3][1] = -10.0\n",
    "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
    "gradients[\"db\"][4] = [10.]\n",
    "gradients[\"dby\"][1] = [8.45833407]\n",
    "\n",
    "Gradients for mValue=5\n",
    "gradients[\"dWaa\"][1][2] = 5.0\n",
    "gradients[\"dWax\"][3][1] = -5.0\n",
    "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
    "gradients[\"db\"][4] = [5.]\n",
    "gradients[\"dby\"][1] = [5.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxSUDRYH01Sg"
   },
   "source": [
    "### Sampling\n",
    "\n",
    "After training the model, generating new text (characters) involves a sampling process, as illustrated below:\n",
    "\n",
    "<img src=\"images/dinos3.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center><b>Figure 3</b>: Illustration of text generation using a trained model. Begin with \n",
    "\n",
    "$x^{\\langle 1 \\rangle} = \\vec{0}$ at the first time step and sample one character at a time from the network.</center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2l1SKw3301Sg"
   },
   "source": [
    "### sample\n",
    "\n",
    "The `sample` function are used to sample characters. \n",
    "\n",
    "There are 4 steps in `sample`:\n",
    "\n",
    "- **Step 1**: Input the \"dummy\" vector of zeros $x^{\\langle 1 \\rangle} = \\vec{0}$. \n",
    "    - This is the default input before generating any characters. \n",
    "      Also, we have, $a^{\\langle 0 \\rangle} = \\vec{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 2**: Run one step of forward propagation to get $a^{\\langle 1 \\rangle}$ and $\\hat{y}^{\\langle 1 \\rangle}$. Here are the equations:\n",
    "\n",
    "*hidden state:*  \n",
    "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t+1 \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}$$\n",
    "\n",
    "*activation:*\n",
    "$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2}$$\n",
    "\n",
    "*prediction:*\n",
    "$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })\\tag{3}$$\n",
    "\n",
    "- Details about $\\hat{y}^{\\langle t+1 \\rangle }$:\n",
    "   - Note that $\\hat{y}^{\\langle t+1 \\rangle }$ is a (softmax) probability vector (its entries are between 0 and 1 and sum to 1). \n",
    "   - $\\hat{y}^{\\langle t+1 \\rangle}_i$ represents the probability that the character indexed by \"i\" is the next character.  \n",
    "   - A `softmax()` function is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_GeJwei01Ss"
   },
   "source": [
    "- **Step 3: Sampling**:\n",
    "    - With $y^{\\langle t+1 \\rangle}$ available, the next task is to select the following letter in the sequence. To introduce variability and avoid generating the same result every time, use `np.random.choice` to select a letter based on its probability, rather than always picking the most probable one.\n",
    "    - Choose the next character's **index** according to the probability distribution given by $\\hat{y}^{\\langle t+1 \\rangle }$. \n",
    "    - For example, if $\\hat{y}^{\\langle t+1 \\rangle }_i = 0.16$, the index \"i\" should be selected with a 16% probability.\n",
    "    - Utilize [np.random.choice](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html) for this task.\n",
    "\n",
    "    Here is an example of how to use `np.random.choice()`:\n",
    "    ```python\n",
    "    np.random.seed(0)\n",
    "    probs = np.array([0.1, 0.0, 0.7, 0.2])\n",
    "    idx = np.random.choice(range(len(probs)), p=probs)\n",
    "    ```\n",
    "\n",
    "    - This results in the index (`idx`) being chosen according to the distribution:\n",
    "    \n",
    "    $P(index = 0) = 0.1, P(index = 1) = 0.0, P(index = 2) = 0.7, P(index = 3) = 0.2$.\n",
    "\n",
    "    - Ensure that the `p` parameter is a 1D vector of probabilities.\n",
    "    - Note that $\\hat{y}^{\\langle t+1 \\rangle}$, referred to as `y` in the code, is a 2D array.\n",
    "    - While implementing, the first argument to `np.random.choice` should be an ordered list `[0, 1, ..., vocab_len-1]`. Avoid using `char_to_ix.values()` as the order of dictionary values might differ between runs, potentially affecting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KHJ4tfWY01Su"
   },
   "source": [
    "- **Step 4: Update $x^{\\langle t \\rangle }$**:\n",
    "    - The final step in the `sample()` function is to update the variable `x`, which currently holds $x^{\\langle t \\rangle }$, with $x^{\\langle t + 1 \\rangle }$.\n",
    "    - Represent $x^{\\langle t + 1 \\rangle }$ by creating a one-hot vector for the character selected as the prediction.\n",
    "    - Forward propagate $x^{\\langle t + 1 \\rangle }$ in Step 1 of the loop and continue this process until a `\"\\n\"` character is generated, signifying the end of the dinosaur name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIkYdtBx01Su"
   },
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- Python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    char_to_ix -- Python dictionary mapping each character to an index.\n",
    "    seed -- Used for grading purposes. Do not worry about it.\n",
    "\n",
    "    Returns:\n",
    "    indices -- A list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    # Step 1: Create the a zero vector x that can be used as the one-hot vector \n",
    "    # Representing the first character (initializing the sequence generation).\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    # Step 1': Initialize a_prev as zeros\n",
    "    a_prev = np.zeros((n_a ,1))\n",
    "    \n",
    "    # Create an empty list of indices. This is the list which will contain the list of indices of the characters to generate\n",
    "    indices = []\n",
    "    \n",
    "    # idx is the index of the one-hot vector x that is set to 1\n",
    "    # All other positions in x are zero.\n",
    "    # Initialize idx to -1\n",
    "    idx = -1\n",
    "    \n",
    "    # Loop over time-steps t. At each time-step:\n",
    "    # Sample a character from a probability distribution \n",
    "    # And append its index (`idx`) to the list \"indices\". \n",
    "    # Stop if it reach 50 characters \n",
    "    # (which should be very unlikely with a well-trained model).\n",
    "    # Setting the maximum number of characters helps with debugging and prevents infinite loops. \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
    "        a = np.tanh(np.dot(Wax,x) + np.dot(Waa,a_prev) + b)\n",
    "        z = np.dot(Wya,a) + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        # For grading purposes\n",
    "        np.random.seed(counter + seed) \n",
    "        \n",
    "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        # (see additional hints above)\n",
    "        idx = np.random.choice(range(len(y)), p = np.squeeze(y) )\n",
    "\n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Step 4: Overwrite the input x with one that corresponds to the sampled index `idx`.\n",
    "        # (see additional hints above)\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "        \n",
    "        # for grading purposes\n",
    "        seed += 1\n",
    "        counter += 1\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Db8r6cXp01Sy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n",
      "list of sampled indices:\n",
      " [23, 16, 26, 26, 24, 3, 21, 1, 7, 24, 15, 3, 25, 20, 6, 13, 10, 8, 20, 12, 2, 0]\n",
      "list of sampled characters:\n",
      " ['w', 'p', 'z', 'z', 'x', 'c', 'u', 'a', 'g', 'x', 'o', 'c', 'y', 't', 'f', 'm', 'j', 'h', 't', 'l', 'b', '\\n']\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "def sample_test(target):\n",
    "    np.random.seed(24)\n",
    "    _, n_a = 20, 100\n",
    "    Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "    b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "\n",
    "    indices = target(parameters, char_to_ix, 0)\n",
    "    print(\"Sampling:\")\n",
    "    print(\"list of sampled indices:\\n\", indices)\n",
    "    print(\"list of sampled characters:\\n\", [ix_to_char[i] for i in indices])\n",
    "    \n",
    "    assert len(indices) < 52, \"Indices length must be smaller than 52\"\n",
    "    assert indices[-1] == char_to_ix['\\n'], \"All samples must end with \\\\n\"\n",
    "    assert min(indices) >= 0 and max(indices) < len(char_to_ix), f\"Sampled indexes must be between 0 and len(char_to_ix)={len(char_to_ix)}\"\n",
    "    assert np.allclose(indices[0:6], [23, 16, 26, 26, 24, 3]), \"Wrong values\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "\n",
    "sample_test(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "Sampling:\n",
    "list of sampled indices:\n",
    " [23, 16, 26, 26, 24, 3, 21, 1, 7, 24, 15, 3, 25, 20, 6, 13, 10, 8, 20, 12, 2, 0]\n",
    "list of sampled characters:\n",
    " ['w', 'p', 'z', 'z', 'x', 'c', 'u', 'a', 'g', 'x', 'o', 'c', 'y', 't', 'f', 'm', 'j', 'h', 't', 'l', 'b', '\\n']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Points\n",
    "\n",
    "* Exploding gradients occur when updates are excessively large, causing them to overshoot the optimal values during backpropagation, which complicates training.\n",
    "    * **Solution:** Clip gradients before updating the parameters to prevent this issue.\n",
    "\n",
    "* Sampling is a technique used to select the index of the next character based on a probability distribution.\n",
    "    * **Steps for character-level sampling:**\n",
    "        * Initialize with a \"dummy\" vector of zeros as the starting input.\n",
    "        * Perform one step of forward propagation to obtain $a^{\\langle 1 \\rangle}$ (the first character) and $\\hat{y}^{\\langle 1 \\rangle}$ (the probability distribution for the next character).\n",
    "        * Use `np.random.choice` to sample from the probability distribution, which introduces variability and prevents repetitive results, making the generated names more interesting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qdeVJ9xT01S2"
   },
   "source": [
    "## Building the Language Model\n",
    "\n",
    "It's time to build the character-level language model for text generation.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "In this section, a function performing one step of stochastic gradient descent with gradient clipping will be implemented. The process involves iterating through the training examples one at a time, applying stochastic gradient descent for optimization.\n",
    "\n",
    "#### Optimization Loop for an RNN\n",
    "\n",
    "The typical optimization loop for an RNN includes the following steps:\n",
    "\n",
    "1. **Forward Propagation:** \n",
    "   - Compute the loss by propagating through the RNN.\n",
    "\n",
    "2. **Backward Propagation Through Time:**\n",
    "   - Calculate the gradients of the loss with respect to the parameters.\n",
    "\n",
    "3. **Gradient Clipping:**\n",
    "   - Clip the gradients to prevent issues with exploding gradients.\n",
    "\n",
    "4. **Parameter Update:**\n",
    "   - Update the parameters using gradient descent based on the clipped gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimize\n",
    "\n",
    "This function implements the optimization process (one step of stochastic gradient descent).\n",
    "\n",
    "The following functions are provided in `utils.py`:\n",
    "`rnn_forward`, `rnn_backward` and `update_parameters`.\n",
    "\n",
    "<!-- ```python\n",
    "def rnn_forward(X, Y, a_prev, parameters):\n",
    "    \"\"\" Performs the forward propagation through the RNN and computes the cross-entropy loss.\n",
    "    It returns the loss' value as well as a \"cache\" storing values to be used in backpropagation.\"\"\"\n",
    "    ....\n",
    "    return loss, cache\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    \"\"\" Performs the backward propagation through time to compute the gradients of the loss with respect\n",
    "    to the parameters. It returns also all the hidden states.\"\"\"\n",
    "    ...\n",
    "    return gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\" Updates parameters using the Gradient Descent Update Rule.\"\"\"\n",
    "    ...\n",
    "    return parameters\n",
    "```\n",
    "\n",
    "Recall that you previously implemented the `clip` function: -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YsaPkvUq01S3"
   },
   "source": [
    "#### Parameters\n",
    "\n",
    "- The `parameters` dictionary, which contains the weights and biases, is updated during optimization. Although `parameters` is not one of the returned values from the `optimize` function, changes made to it within the function persist outside of it. This is because `parameters` is passed by reference.\n",
    "- In Python, dictionaries and lists are passed by reference. Thus, if a dictionary or list is passed to a function and modified within that function, the changes are applied to the original dictionary or list, not a copy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BbEdIgY01S3"
   },
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    # Read the descriptions for the following functions in utils.py\n",
    "    # Forward propagate through time\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    # Backpropagate through time\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max)\n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    # Update parameters\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ww-EKK5801S5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 126.50397572165389\n",
      "gradients[\"dWaa\"][1][2] = 0.19470931534713587\n",
      "np.argmax(gradients[\"dWax\"]) = 93\n",
      "gradients[\"dWya\"][1][2] = -0.007773876032002162\n",
      "gradients[\"db\"][4] = [-0.06809825]\n",
      "gradients[\"dby\"][1] = [0.01538192]\n",
      "a_last[4] = [-1.]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "def optimize_test(target):\n",
    "    np.random.seed(1)\n",
    "    vocab_size, n_a = 27, 100\n",
    "    a_prev = np.random.randn(n_a, 1)\n",
    "    Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "    b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "    X = [12, 3, 5, 11, 22, 3]\n",
    "    Y = [4, 14, 11, 22, 25, 26]\n",
    "    old_parameters = copy.deepcopy(parameters)\n",
    "    loss, gradients, a_last = target(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "    print(\"Loss =\", loss)\n",
    "    print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "    print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
    "    print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "    print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "    print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
    "    print(\"a_last[4] =\", a_last[4])\n",
    "    \n",
    "    assert np.isclose(loss, 126.5039757), \"Problems with the call of the rnn_forward function\"\n",
    "    for grad in gradients.values():\n",
    "        assert np.min(grad) >= -5, \"Problems in the clip function call\"\n",
    "        assert np.max(grad) <= 5, \"Problems in the clip function call\"\n",
    "    assert np.allclose(gradients['dWaa'][1, 2], 0.1947093), \"Unexpected gradients. Check the rnn_backward call\"\n",
    "    assert np.allclose(gradients['dWya'][1, 2], -0.007773876), \"Unexpected gradients. Check the rnn_backward call\"\n",
    "    assert not np.allclose(parameters['Wya'], old_parameters['Wya']), \"parameters were not updated\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "\n",
    "optimize_test(optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "```\n",
    "Loss = 126.50397572165389\n",
    "gradients[\"dWaa\"][1][2] = 0.19470931534713587\n",
    "np.argmax(gradients[\"dWax\"]) = 93\n",
    "gradients[\"dWya\"][1][2] = -0.007773876032002162\n",
    "gradients[\"db\"][4] = [-0.06809825]\n",
    "gradients[\"dby\"][1] = [0.01538192]\n",
    "a_last[4] = [-1.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-4cL3oZ01S9"
   },
   "source": [
    "### Training the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZ63mN5-01S9"
   },
   "source": [
    "* Given the dataset of dinosaur names, you'll use each line of the dataset (one name) as one training example. \n",
    "* Every 2000 steps of stochastic gradient descent, you will sample several randomly chosen names to see how the algorithm is doing. \n",
    " \n",
    "### model\n",
    "\n",
    "When `examples[index]` contains one dinosaur name (string), to create an example (X, Y), we can use this:\n",
    "\n",
    "##### Set the index `idx` into the list of examples\n",
    "\n",
    "- Iterate through the shuffled list of dinosaur names in the \"examples\" list using a for-loop.\n",
    "- If there are `n_e` examples and the index exceeds `n_e`, the index should cycle back to 0. This ensures continuous feeding of examples into the model when `j` is `n_e`, `n_e + 1`, etc.\n",
    "- Hint: `(n_e + 1) % n_e` equals 1, which represents the remainder when `(n_e + 1)` is divided by `n_e`.\n",
    "- `%` is the [modulo operator in Python](https://www.freecodecamp.org/news/the-python-modulo-operator-what-does-the-symbol-mean-in-python-solved/).\n",
    "\n",
    "##### Extract a single example from the list of examples\n",
    "- Use the previously set `idx` index to retrieve one word from the \"examples\" list, referred to as `single_example`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNDsHeoQ01S9"
   },
   "source": [
    "##### Convert a string into a list of characters: `single_example_chars`\n",
    "- `single_example_chars`: A string is a list of characters.\n",
    "- Use a list comprehension (recommended over for-loops) to generate a list of characters.\n",
    "\n",
    "```Python\n",
    "str = 'I love deep learning'\n",
    "list_of_chars = [c for c in str]\n",
    "print(list_of_chars)\n",
    "```\n",
    "\n",
    "```\n",
    "['I', ' ', 'l', 'o', 'v', 'e', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g']\n",
    "```\n",
    "\n",
    "* For more on [list comprehensions](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzSvzjde01S-"
   },
   "source": [
    "##### Convert list of characters to a list of integers: `single_example_ix`\n",
    "* Create a list that contains the index numbers associated with each character.\n",
    "* Use the dictionary `char_to_ix`\n",
    "* Combine this with the list comprehension that is used to get a list of characters from a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lL05MmsV01S-"
   },
   "source": [
    "##### Create the list of input characters: `X`\n",
    "* `rnn_forward` uses the **`None`** value as a flag to set the input vector as a zero-vector.\n",
    "* Prepend the list [**`None`**] in front of the list of input character *indices*.\n",
    "* There is more than one way to prepend a value to a list.  One way is to add two lists together: `['a'] + ['b']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2D8OXGv901S-"
   },
   "source": [
    "##### Get the integer representation of the newline character `ix_newline`\n",
    "* `ix_newline`: The newline character signals the end of the dinosaur name.\n",
    "    - Get the integer representation of the newline character `'\\n'`.\n",
    "    - Use `char_to_ix`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGnawmFe01S_"
   },
   "source": [
    "##### Set the list of labels (integer representation of the characters): `Y`\n",
    "* The goal is to train the RNN to predict the next letter in the name, so the labels are the list of characters that are one time-step ahead of the characters in the input `X`.\n",
    "    - For example, `Y[0]` contains the same value as `X[1]`  \n",
    "* The RNN should predict a newline at the last letter, so add `ix_newline` to the end of the labels. \n",
    "    - Append the integer representation of the newline character to the end of `Y`.\n",
    "    - Note that `append` is an in-place operation.\n",
    "    - It might be easier for you to add two lists together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l214uNun01S_"
   },
   "outputs": [],
   "source": [
    "def model(data_x, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27, verbose = False):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "    \n",
    "    Arguments:\n",
    "    data_x -- text corpus, divided in words\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text (size of the vocabulary)\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    examples = [x.strip() for x in data_x]\n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # for grading purposes\n",
    "    last_dino_name = \"abc\"\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        # Set the index `idx` (see instructions above)\n",
    "        idx = j % len(examples)\n",
    "        \n",
    "        # Set the input X (see instructions above)\n",
    "        single_example_chars = examples[idx]\n",
    "        single_example_ix = [char_to_ix[c] for c in single_example_chars]\n",
    "        X = [None] + single_example_ix\n",
    "        \n",
    "        # Set the labels Y (see instructions above)\n",
    "        ix_newline = [char_to_ix[\"\\n\"]]\n",
    "        Y = X[1:] + ix_newline\n",
    "\n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "        \n",
    "        # debug statements to aid in correctly forming X, Y\n",
    "        if verbose and j in [0, len(examples) -1, len(examples)]:\n",
    "            print(\"j = \" , j, \"idx = \", idx,) \n",
    "        if verbose and j in [0]:\n",
    "#             print(\"single_example =\", single_example)\n",
    "            print(\"single_example_chars\", single_example_chars)\n",
    "            print(\"single_example_ix\", single_example_ix)\n",
    "            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n",
    "        \n",
    "        # to keep the loss smooth.\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            # The number of dinosaur names to print\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                last_dino_name = get_sample(sampled_indices, ix_to_char)\n",
    "                print(last_dino_name.replace('\\n', ''))\n",
    "                \n",
    "                seed += 1  # To get the same result (for grading purposes), increment the seed by one. \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters, last_dino_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HvBlAnS01TB"
   },
   "source": [
    "When you run the following cell, you should observe your model outputting random-looking characters at the first iteration. After a few thousand iterations, your model should learn to generate reasonable-looking names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3EH8Edc001TC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j =  0 idx =  0\n",
      "single_example_chars turiasaurus\n",
      "single_example_ix [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19]\n",
      " X =  [None, 20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19] \n",
      " Y =        [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19, 0] \n",
      "\n",
      "Iteration: 0, Loss: 23.087336\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "j =  1535 idx =  1535\n",
      "j =  1536 idx =  0\n",
      "Iteration: 2000, Loss: 27.884160\n",
      "\n",
      "Liusskeomnolxeros\n",
      "Hmdaairus\n",
      "Hytroligoraurus\n",
      "Lecalosapaus\n",
      "Xusicikoraurus\n",
      "Abalpsamantisaurus\n",
      "Tpraneronxeros\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.901815\n",
      "\n",
      "Mivrosaurus\n",
      "Inee\n",
      "Ivtroplisaurus\n",
      "Mbaaisaurus\n",
      "Wusichisaurus\n",
      "Cabaselachus\n",
      "Toraperlethosdarenitochusthiamamumamaon\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.608779\n",
      "\n",
      "Onwusceomosaurus\n",
      "Lieeaerosaurus\n",
      "Lxussaurus\n",
      "Oma\n",
      "Xusteonosaurus\n",
      "Eeahosaurus\n",
      "Toreonosaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.070350\n",
      "\n",
      "Onxusichepriuon\n",
      "Kilabersaurus\n",
      "Lutrodon\n",
      "Omaaerosaurus\n",
      "Xutrcheps\n",
      "Edaksoje\n",
      "Trodiktonus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.844446\n",
      "\n",
      "Onyusaurus\n",
      "Klecalosaurus\n",
      "Lustodon\n",
      "Ola\n",
      "Xusodonia\n",
      "Eeaeosaurus\n",
      "Troceosaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.291971\n",
      "\n",
      "Onyxosaurus\n",
      "Kica\n",
      "Lustrepiosaurus\n",
      "Olaagrraiansaurus\n",
      "Yuspangosaurus\n",
      "Eealosaurus\n",
      "Trognesaurus\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.382338\n",
      "\n",
      "Meutromodromurus\n",
      "Inda\n",
      "Iutroinatorsaurus\n",
      "Maca\n",
      "Yusteratoptititan\n",
      "Ca\n",
      "Troclosaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.224544\n",
      "\n",
      "Meusspanchodtashuarhiaspantaxia\n",
      "Indaa\n",
      "Iuspsauhosaurus\n",
      "Macacosaurus\n",
      "Yusoconikaulrit\n",
      "Cacasoceimurus\n",
      "Trrasaurus\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 22.904954\n",
      "\n",
      "Pivrrong\n",
      "Llecanosaurus\n",
      "Myssocilindus\n",
      "Peeaishidanagtallsaurus\n",
      "Ytrong\n",
      "Eg\n",
      "Trojichus\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 23.005394\n",
      "\n",
      "Nkytrohelosaurus\n",
      "Lolaagosaurus\n",
      "Lyusochosaurus\n",
      "Necakson\n",
      "Yussangosaurus\n",
      "Eiagosaurus\n",
      "Trodon\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 22.728886\n",
      "\n",
      "Onustreofkelus\n",
      "Llecagosaurus\n",
      "Mystolojmiaterltasaurus\n",
      "Ola\n",
      "Yuskeolongus\n",
      "Eiacosaurus\n",
      "Trodonosaurus\n",
      "\n",
      "\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "parameters, last_name = model(data.split(\"\\n\"), ix_to_char, char_to_ix, 22001, verbose = True)\n",
    "\n",
    "assert last_name == 'Trodonosaurus\\n', \"Wrong expected output\"\n",
    "print(\"\\033[92mAll tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "...\n",
    "Iteration: 22000, Loss: 22.728886\n",
    "\n",
    "Onustreofkelus\n",
    "Llecagosaurus\n",
    "Mystolojmiaterltasaurus\n",
    "Ola\n",
    "Yuskeolongus\n",
    "Eiacosaurus\n",
    "Trodonosaurus\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4mmYZwFu01TF"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "The algorithm has started to generate plausible dinosaur names towards the end of training. Initially, it produced random characters, but over time, recognizable dinosaur names with interesting endings emerged. Running the algorithm longer and adjusting hyperparameters might yield even better results! Names like `maconucon`, `marloralus`, and `macingsersaurus` were generated, indicating that the model has learned patterns such as dinosaur names ending in `saurus`, `don`, `aura`, `tor`, etc.\n",
    "\n",
    "If some generated names are not cool, it's worth noting that not all real dinosaur names are either (e.g., `dromaeosauroides` is a real name from the training set). The model provides a set of candidates, allowing selection of the coolest names.\n",
    "\n",
    " A small dataset is used for quick training on a CPU. Training a model for the English language requires a larger dataset, more computation, and longer training times on GPUs. Despite the brief training, names like the great and fierce **Mangosaurus** were among the favorites!\n",
    "\n",
    "<img src=\"images/mangosaurus.jpeg\" style=\"width:250;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2u55TY-W01TG"
   },
   "source": [
    "## Writing like Shakespeare\n",
    "\n",
    "A similar task to character-level text generation (but more complex) is generating Shakespearean poems. Instead of learning from a dataset of dinosaur names, a collection of Shakespearean poems can be used. With LSTM cells, longer-term dependencies spanning many characters in the text can be learned—for example, a character appearing at one point in a sequence can influence what appears much later. These long-term dependencies were less critical with dinosaur names, as the names are relatively short.\n",
    "\n",
    "<img src=\"images/shakespeare.jpg\" style=\"width:500;height:400px;\">\n",
    "<caption><center><b>Let's become poets!</b></center></caption>\n",
    "\n",
    "Below, a Shakespeare poem generator with Keras can be implemented. Run the following cell to load the required packages and models. This may take a few minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgPMYCZl01TH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "Creating training set...\n",
      "number of training examples: 31412\n",
      "Vectorizing training set...\n",
      "Loading model...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from shakespeare_utils import *\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jRaDcXrU01TJ"
   },
   "source": [
    "To save you some time, a model has already been trained for ~1000 epochs on a collection of Shakespearean poems called \"<i>[The Sonnets](shakespeare.txt)</i>.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XGG0VdGv01TJ"
   },
   "source": [
    "Train the model for one more epoch. After the training completes (this will take a few minutes), run `generate_output`. This will prompt for an input (less than 40 characters). The poem will start with the provided sentence, and the RNN Shakespeare will complete the rest of the poem. For example, try \"Forsooth this maketh no sense\" (without the quotation marks). The results may vary depending on whether a space is included at the end, so experiment with both options and try other inputs as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4DBjTlz01TK",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 123s 501ms/step - loss: 2.5729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3a5802c990>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDoriy4Z01TM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: The model \n",
      "\n",
      "\n",
      "Here is your poem: \n",
      "\n",
      "The model mins ans ele suland:\n",
      "to praviout unlath or to mr fill fer,\n",
      "why cofdol's my evine sue best even thy poy,\n",
      "and my betanch worls dither bame enredces,\n",
      "on thises wo should of lend ally heaor,\n",
      "with a credy as i conlest her will new.\n",
      "thy mower doth to dithath shume's levery?\n",
      "bet ctronga enleanes that is new she that,\n",
      "whone eyes and well veose thicht chlight,\n",
      "when is a were best autthrrongs yew wascuts cr"
     ]
    }
   ],
   "source": [
    "# Run this cell to try with different inputs without having to re-train the model \n",
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1UA61-t01TO"
   },
   "source": [
    "The RNN Shakespeare model is very similar to the one you built for dinosaur names. The only major differences are:\n",
    "- LSTMs instead of the basic RNN to capture longer-range dependencies\n",
    "- The model is a deeper, stacked LSTM model (2 layer)\n",
    "- Using Keras instead of Python to simplify the code \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2EgCyjte01TP"
   },
   "source": [
    "<a name='5'></a>\n",
    "## 5 - References \n",
    "- This exercise took inspiration from Andrej Karpathy's implementation: https://gist.github.com/karpathy/d4dee566867f8291f086. To learn more about text generation, also check out Karpathy's [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Dinosaurus_Island_Character_level_language_model_final_v3b+Proposed.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "DLSC5W1-A2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
