{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbzZLqIPv6b7",
    "outputId": "19f2fc2b-6f1d-4b43-fd50-4c513e3936fd"
   },
   "source": [
    "# Transformer Pre-processing\n",
    "\n",
    "This notebook, adapted from Deeplearning.ai's Deep Learning course, explores the pre-processing methods applied to raw text before it is passed to the encoder and decoder blocks of the transformer architecture.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Create visualizations to gain intuition on positional encodings\n",
    "- Visualize how positional encodings affect word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OpwqWL2QH5G"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Here are the positional encoding equations:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "In natural language processing tasks, it is common practice to convert sentences into tokens before inputting them into a language model. Each token is then represented as a fixed-length numerical vector called an embedding, which encapsulates the meaning of the words. In the Transformer architecture, a positional encoding vector is added to the embedding to convey positional information throughout the model.\n",
    "\n",
    "Understanding these vectors can be challenging when only numerical representations are examined. However, visualizations can provide insight into the semantic and positional relationships between words. Reducing embeddings to two dimensions and plotting them shows that semantically similar words cluster together, while dissimilar words are spaced further apart. Similarly, positional encoding vectors can be visualized to reveal that words closer together in a sentence appear closer on a Cartesian plane, while those farther apart appear more distant.\n",
    "\n",
    "In this notebook, a series of visualizations will be created to explore word embeddings and positional encoding vectors, aiming to illustrate how positional encodings impact word embeddings and convey sequential information through the Transformer architecture.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding Visualizations\n",
    "\n",
    "The next code cell includes the `positional_encoding` function that was implemented previously. This notebook will build upon that work to create additional visualizations using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y78txxoHQtwG"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        positions (int) -- Maximum number of positions to be encoded \n",
    "        d (int) -- Encoding size \n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding -- (1, position, d_model) A matrix with the positional encodings\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize a matrix angle_rads of all the angles \n",
    "    angle_rads = np.arange(positions)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d)[np.newaxis, :]//2)) / np.float32(d))\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the embedding dimension as 100, which should match the dimensionality of the word embeddings. In the [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) paper, embedding sizes range from 100 to 1024 depending on the task, with maximum sequence lengths varying from 40 to 512. For this notebook:\n",
    "\n",
    "- Set the maximum sequence length to 100\n",
    "- Set the maximum number of words to 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NB_WORDS = 64\n",
    "pos_encoding = positional_encoding(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('d')\n",
    "plt.xlim((0, EMBEDDING_DIM))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've previously created this visualization, but let's explore it further. Observe some interesting properties of the matrix: notably, the norm of each vector remains constant. Regardless of the value of `pos`, the norm always equals 7.071068. This constancy implies that the dot product of two positional encoding vectors is unaffected by the scale of the vector, which is significant for correlation calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 34\n",
    "tf.norm(pos_encoding[0,pos,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another noteworthy property is that the norm of the difference between two vectors separated by `k` positions remains constant. When `k` is fixed and `pos` varies, the difference remains approximately the same. This characteristic highlights that the difference is determined by the relative separation between encodings rather than their absolute positions. Expressing positional encodings as linear functions of one another can aid the model in focusing on the relative positions of words.\n",
    "\n",
    "Achieving this representation of word position differences through vector encodings is challenging, particularly because the values of these encodings must be small enough to avoid distorting the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 70\n",
    "k = 2\n",
    "print(tf.norm(pos_encoding[0,pos,:] -  pos_encoding[0,pos + k,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having observed some interesting properties of the positional encoding vectors, the next step is to create visualizations to explore how these properties influence the relationships between encodings and embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Positional Encodings\n",
    "\n",
    "#### Correlation\n",
    "\n",
    "The positional encoding matrix provides insight into the uniqueness of each vector for every position. However, it remains unclear how these vectors represent the relative positions of words within a sentence. To clarify this, calculate the correlation between pairs of vectors at each position. An effective positional encoder will generate a symmetric matrix where the highest values are found along the main diagonalâ€”vectors at similar positions should exhibit the highest correlation. Accordingly, correlation values are expected to decrease as they move away from the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoding correlation\n",
    "corr = tf.matmul(pos_encoding, pos_encoding, transpose_b=True).numpy()[0]\n",
    "plt.pcolormesh(corr, cmap='RdBu')\n",
    "plt.xlabel('Position')\n",
    "plt.xlim((0, MAX_SEQUENCE_LENGTH))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean Distance\n",
    "\n",
    "Alternatively, the Euclidean distance can be used to compare the positional encoding vectors. In this approach, the visualization will show a matrix where the main diagonal has values of 0, and the off-diagonal values increase as they move away from the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoding euclidean distance\n",
    "eu = np.zeros((MAX_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH))\n",
    "print(eu.shape)\n",
    "for a in range(MAX_SEQUENCE_LENGTH):\n",
    "    for b in range(a + 1, MAX_SEQUENCE_LENGTH):\n",
    "        eu[a, b] = tf.norm(tf.math.subtract(pos_encoding[0, a], pos_encoding[0, b]))\n",
    "        eu[b, a] = eu[a, b]\n",
    "        \n",
    "plt.pcolormesh(eu, cmap='RdBu')\n",
    "plt.xlabel('Position')\n",
    "plt.xlim((0, MAX_SEQUENCE_LENGTH))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Embedding\n",
    "\n",
    "Insights into the relationships between positional encoding vectors and other vectors at different positions have been gained through the creation of correlation and distance matrices. To further understand how positional encodings impact word embeddings, visualize the sum of these vectors for a clearer perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Embedding\n",
    "\n",
    "To integrate a pretrained word embedding with the positional encodings, begin by loading an embedding from the [GloVe](https://nlp.stanford.edu/projects/glove/) project. The pretrained embeddings file can be downloaded from [this link](https://www.kaggle.com/datasets/danielwillgeorge/glove6b100dtxt). We will use the embedding with 100 features for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "# put the downloaded glove file in the same directory as this script\n",
    "# or change the path accordingly\n",
    "GLOVE_DIR = \"glove\"\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "print('d_model:', embeddings_index['hi'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This embedding is composed of 400,000 words and each word embedding has 100 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following text, which contains just two sentences. Note that these sentences are constructed to illustrate specific points:\n",
    "\n",
    "* Each sentence is made up of word sets with semantic similarities within each group.\n",
    "* In the first sentence, similar terms are placed consecutively, whereas in the second sentence, the order is random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['king queen man woman dog wolf football basketball red green yellow',\n",
    "         'man queen yellow basketball green dog  woman football  king red wolf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run the following code cell to apply tokenization to the raw text. While the details of this step will be covered in later ungraded labs, hereâ€™s a brief overview (not crucial for understanding the current lab):\n",
    "\n",
    "* The code processes an array of plain text with varying sentence lengths and produces a matrix where each row corresponds to a sentence, represented as an array of size `MAX_SEQUENCE_LENGTH`.\n",
    "* Each value in this array represents a word from the sentence, indexed according to a dictionary (`word_index`).\n",
    "* Sequences shorter than `MAX_SEQUENCE_LENGTH` are padded with zeros to ensure uniform length.\n",
    "\n",
    "Detailed explanations will follow in subsequent ungraded labs, so thereâ€™s no need to focus too much on this step right now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, padding='post', maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To streamline the model, focus on obtaining embeddings for only the distinct words present in the text being examined. In this case, filter out the embeddings for the 11 specific words found in the sentences. The first vector will be an array of zeros, representing all unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an embedding layer using the weights extracted from the pretrained glove embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the input tokenized data to the embedding using the previous layer. Check the shape of the embedding to make sure the last dimension of this matrix contains the embeddings of the words in the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embedding_layer(data)\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization on a Cartesian Plane\n",
    "\n",
    "Next, create a function to visualize the word encodings on a Cartesian plane. This will involve using PCA to reduce the 100-dimensional GloVe embeddings to just 2 components for easier visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_words(embedding, sequences, sentence):\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca_train = pca.fit_transform(embedding[sentence,0:len(sequences[sentence]),:])\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6)) \n",
    "    plt.rcParams['font.size'] = '12'\n",
    "    ax.scatter(X_pca_train[:, 0], X_pca_train[:, 1])\n",
    "    words = list(word_index.keys())\n",
    "    for i, index in enumerate(sequences[sentence]):\n",
    "        ax.annotate(words[index-1], (X_pca_train[i, 0], X_pca_train[i, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the embedding of each of the sentences. Each plot should disply the embeddings of the different words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_words(embedding, sequences, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the word embeddings of the second sentence, which contains the same words as the first sentence but in a different order. This visualization will demonstrate that the order of the words does not impact their vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_words(embedding, sequences, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic and Positional Embeddings\n",
    "\n",
    "Next, combine the original GloVe embeddings with the positional encodings calculated earlier. For this exercise, use a 1-to-1 weight ratio between the semantic and positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding2 = embedding * 1.0 + pos_encoding[:,:,:] * 1.0\n",
    "\n",
    "plot_words(embedding2, sequences, 0)\n",
    "plot_words(embedding2, sequences, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe significant differences between the plots. Both plots have undergone drastic changes compared to their original versions. In the second image, which represents the sentence where similar words are not grouped together, we can observe that very dissimilar words like `red` and `wolf` appear closer together.\n",
    "\n",
    "Experiment with different relative weights to see how they strongly influence the vector representations of the words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = 1 # Change me\n",
    "W2 = 10 # Change me\n",
    "embedding2 = embedding * W1 + pos_encoding[:,:,:] * W2\n",
    "plot_words(embedding2, sequences, 0)\n",
    "plot_words(embedding2, sequences, 1)\n",
    "\n",
    "# For reference\n",
    "#['king queen man woman dog wolf football basketball red green yellow',\n",
    "# 'man queen yellow basketball green dog  woman football  king red wolf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `W1 = 1` and `W2 = 10`, the arrangement of the words will start to exhibit a clockwise or counterclockwise pattern, depending on their positions in the sentence. With these parameters, the positional encoding vectors will dominate the embeddings.\n",
    "\n",
    "Now, try using `W1 = 10` and `W2 = 1`. Under these conditions, the plot will closely resemble the original embedding visualizations, with only minor changes in the positions of the words.\n",
    "\n",
    "In the previous Transformer notebook, the word embedding was multiplied by `sqrt(EMBEDDING_DIM)`. In this case, using `W1 = sqrt(EMBEDDING_DIM) = 10` and `W2 = 1` will be equivalent.\n",
    "\n",
    "#### Recap\n",
    "- Positional encodings can be expressed as linear functions of each other, allowing the model to learn based on the relative positions of words.\n",
    "- While positional encodings can influence word embeddings, a small relative weight for the positional encoding will preserve the semantic meaning of the words."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transformer Assignment - Subclass.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
